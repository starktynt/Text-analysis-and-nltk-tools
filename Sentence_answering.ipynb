{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence_answering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0SFai8w_c9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecaa9f7b-1911-4723-cb48-2480952c6c8f"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import string\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def decode_response(test_input):\n",
        "    #Getting the output states to pass into the decoder\n",
        "    states_value = encoder_model.predict(test_input)\n",
        "    #Generating empty target sequence of length 1\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    #Setting the first token of target sequence with the start token\n",
        "    target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
        "    \n",
        "    #A variable to store our response word by word\n",
        "    decoded_sentence = ''\n",
        "    \n",
        "\"\"\"\n",
        "      #Predicting output tokens with probabilities and states\n",
        "      output_tokens, hidden_state, cell_state = decoder_model.predict([target_seq] + states_value)#Choosing the one with highest probability\n",
        "      sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "      sampled_token = reverse_target_features_dict[sampled_token_index]\n",
        "      decoded_sentence += \" \" + sampled_token#Stop if hit max length or found the stop token\n",
        "      if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "        stop_condition = True#Update the target sequence\n",
        "      target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "      target_seq[0, 0, sampled_token_index] = 1.\n",
        "      #Update states\n",
        "      states_value = [hidden_state, cell_state]return decoded_sentence\n",
        "\"\"\"\n",
        "\n",
        "raw_html = urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Osteoarthritis#Signs_and_symptoms\")\n",
        "\n",
        "\n",
        "raw_html = raw_html.read()\n",
        "\n",
        "\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "article_text = ''\n",
        "\n",
        "for para in article_paragraphs:\n",
        "    article_text += para.text\n",
        "\n",
        "\n",
        "article_text = article_text.lower()\n",
        "\n",
        "\n",
        "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
        "article_text = re.sub(r'\\s+', ' ', article_text)\n",
        "type(article_paragraphs)\n",
        "\n",
        "\n",
        "article_sentences = nltk.sent_tokenize(article_text)\n",
        "article_words = nltk.word_tokenize(article_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ChatBot:\n",
        "  negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\n",
        "  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
        "  def start_chat(self):\n",
        "    user_response = input(\"Hi, I'm a chatbot trained on random dialogs. Would you like to chat with me?\\n\")\n",
        "    \n",
        "    if user_response in self.negative_responses:\n",
        "      print(\"Ok, have a great day!\")\n",
        "      return\n",
        "    self.chat(user_response)\n",
        "  def chat(self, reply):\n",
        "    while not self.make_exit(reply):\n",
        "      reply = input(self.generate_response(reply)+\"\\n\")\n",
        "    \n",
        " \n",
        "  def string_to_matrix(self, user_input):\n",
        "    tokens = re.findall(r\"[\\w']+|[^\\s\\w]\", user_input)\n",
        "    user_input_matrix = np.zeros(\n",
        "      (1, max_encoder_seq_length, num_encoder_tokens),\n",
        "      dtype='float32')\n",
        "    for timestep, token in enumerate(tokens):\n",
        "      if token in input_features_dict:\n",
        "        user_input_matrix[0, timestep, input_features_dict[token]] = 1.\n",
        "    return user_input_matrix\n",
        "  \n",
        "  #Method that will create a response using seq2seq model we built\n",
        "  def generate_response(self, user_input):\n",
        "    input_matrix = self.string_to_matrix(user_input)\n",
        "    chatbot_response = decode_response(input_matrix)\n",
        "    \n",
        "    chatbot_response = chatbot_response.replace(\"<START>\",'')\n",
        "    chatbot_response = chatbot_response.replace(\"<END>\",'')\n",
        "    return chatbot_response\n",
        "  def make_exit(self, reply):\n",
        "    for exit_command in self.exit_commands:\n",
        "      if exit_command in reply:\n",
        "        print(\"Ok, have a great day!\")\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "def app(arg):\n",
        "  raw_html = urllib.request.urlopen(arg)\n",
        "\n",
        "\n",
        "  raw_html = raw_html.read()\n",
        "\n",
        "\n",
        "  article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "  article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "  article_text = ''\n",
        "\n",
        "  for para in article_paragraphs:\n",
        "    article_text += para.text\n",
        "\n",
        "  article_text = str(article_text)\n",
        "  article_text = article_text.lower()\n",
        "\n",
        "\n",
        "  article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
        "  article_text = re.sub(r'\\s+', ' ', article_text)\n",
        "  article_sentences = nltk.sent_tokenize(article_text)\n",
        "  article_words = nltk.word_tokenize(article_text)\n",
        "  #print(article_text)\n",
        "  return (article_words)\n",
        "\n",
        "def app_sentence(arg):\n",
        "  raw_html = urllib.request.urlopen(arg)\n",
        "\n",
        "\n",
        "  raw_html = raw_html.read()\n",
        "\n",
        "\n",
        "  article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "  article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "  article_text = ''\n",
        "\n",
        "  for para in article_paragraphs:\n",
        "    article_text += para.text\n",
        "\n",
        "  article_text = str(article_text)\n",
        "  article_text = article_text.lower()\n",
        "\n",
        "\n",
        "  article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
        "  article_text = re.sub(r'\\s+', ' ', article_text)\n",
        "  article_sentences = nltk.sent_tokenize(article_text)\n",
        "  article_words = nltk.word_tokenize(article_text)\n",
        "  #print(article_text)\n",
        "  return (article_sentences)\n",
        "\n",
        "\n",
        "def app_textm(arg):\n",
        "  raw_html = urllib.request.urlopen(arg)\n",
        "  raw_html = raw_html.read()\n",
        "\n",
        "  article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "  article_paragraphs = article_html.find_all('p')\n",
        "  article_text = ''\n",
        "\n",
        "  for para in article_paragraphs:\n",
        "    article_text += para.text\n",
        "  \n",
        "  article_text = str(article_text)\n",
        "  article_text = article_text.lower()\n",
        "\n",
        "  article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
        "  article_text = re.sub(r'\\s+', ' ', article_text)\n",
        "  article_sentences = nltk.sent_tokenize(article_text)\n",
        "  article_words = nltk.word_tokenize(article_text)\n",
        "  #print(article_text)\n",
        "  return (article_text)\n",
        "\n",
        "\n",
        "\n",
        "article_text   =  article_text + app_textm(\"https://en.wikipedia.org/wiki/Bone_tumor#Symptoms\")  + app_textm(\"https://en.wikipedia.org/wiki/Bone_remodeling\") + app_textm(\"https://en.wikipedia.org/wiki/Osteopetrosis\")\n",
        "\n",
        "article_words = article_words + app(\"https://en.wikipedia.org/wiki/Bone_tumor#Symptoms\") + app(\"https://en.wikipedia.org/wiki/Bone_remodeling\") + app(\"https://en.wikipedia.org/wiki/Osteopetrosis\")\n",
        "\n",
        "article_sentences = article_sentences + app_sentence(\"https://en.wikipedia.org/wiki/Bone_tumor#Symptoms\") +app_sentence(\"https://en.wikipedia.org/wiki/Bone_remodeling\")+app_sentence(\"https://en.wikipedia.org/wiki/Osteopetrosis\")\n",
        "\n",
        "\n",
        "\n",
        "######NLP|LEMATIZATION\n",
        "\n",
        "\n",
        "\n",
        "wnlemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def perform_lemmatization(tokens):\n",
        "    return [wnlemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "punctuation_removal = dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
        "\n",
        "def get_processed_text(document):\n",
        "    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))\n",
        "\n",
        "greeting_inputs = (\"hey\", \"HI\", \"good morning\", \"good evening\", \"morning\", \"evening\", \"hi\", \"whatsup\",\"hello\",\"howdy\",\"you fine\",\"what are you doing\")\n",
        "greeting_responses = [\"Hey\", \"Hey Hows you?\", \"*Nods*\", \"Hello, How you doing\", \"Hello\", \"Welcome, I am good and you\",\"Hello Ask me something \",\"Hey back bro\"]\n",
        "\n",
        "def generate_greeting_response(greeting):\n",
        "    for token in greeting.split():\n",
        "        if token.lower() in greeting_inputs:\n",
        "            return random.choice(greeting_responses)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def generate_response(user_input):\n",
        "    tennisrobo_response = ''\n",
        "    article_sentences.append(user_input)\n",
        "\n",
        "    word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')\n",
        "    all_word_vectors = word_vectorizer.fit_transform(article_sentences)\n",
        "    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
        "    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
        "\n",
        "    matched_vector = similar_vector_values.flatten()\n",
        "    matched_vector.sort()\n",
        "    vector_matched = matched_vector[-2]\n",
        "\n",
        "    if vector_matched == 0:\n",
        "        tennisrobo_response = tennisrobo_response + \"I am sorry ? No definetive match :p\"\n",
        "        return tennisrobo_response\n",
        "    else:\n",
        "        tennisrobo_response = tennisrobo_response + article_sentences[similar_sentence_number]\n",
        "        return tennisrobo_response\n",
        "\n",
        "nltk.download('wordnet')\n",
        "word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')\n",
        "all_word_vectors = word_vectorizer.fit_transform(article_sentences)\n",
        "\n",
        "similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
        "\n",
        "similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
        "\n",
        "continue_dialogue = True\n",
        "lsq = []\n",
        "lsa = []\n",
        "\n",
        "while(continue_dialogue == True):\n",
        "    human_text = input(\"Enter the Question - \")\n",
        "    \n",
        "    human_text = human_text.lower()\n",
        "    if human_text != 'Bye':\n",
        "        if human_text == 'thanks' or human_text == 'thank you very much' or human_text == 'quit'or human_text == 'exit':\n",
        "            continue_dialogue = False\n",
        "            print(\"Answer : Most Welcome !\")\n",
        "        else:\n",
        "            if generate_greeting_response(human_text) != None:\n",
        "                print(\"Answer: \" + generate_greeting_response(human_text))\n",
        "            else:\n",
        "                print(\"Answer: \", end=\"\")\n",
        "                dd = generate_response(human_text)\n",
        "                print(dd)\n",
        "                lsq.append(human_text)\n",
        "                lsa.append(dd)\n",
        "                #article_sentences.remove(human_text)\n",
        "    else:\n",
        "        continue_dialogue = False\n",
        "        print(\"Your Anwer : Good bye and take care of yourself...\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Enter the Question - what is bone cancer\n",
            "Answer: stage 1a bone cancer stage 1b bone cancer stage 2a bone cancer stage 2b bone cancer stage 3 bone cancer treatment of bone tumors is highly dependent on the type of tumor.\n",
            "Enter the Question - what is harmful for health\n",
            "Answer: a 2014 cochrane review found that while asu might help relieve pain in the short term for some people with osteoarthritis, it does not appear to improve or maintain the health of affected joints.\n",
            "Enter the Question - are tumors cure\n",
            "Answer: with malignant bone tumors that have not spread, most patients achieve a cure, but the cure rate depends on the type of cancer, location, size, and other factors.\n",
            "Enter the Question - bye\n",
            "Answer: I am sorry ? No definetive match :p\n",
            "Enter the Question - exit\n",
            "Answer : Most Welcome !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AypOMi1tvbaL",
        "outputId": "4a10b487-8e07-4be1-e1de-8b6290ce4c66"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "  \"Question\": [],\n",
        "  \"Ans\": []\n",
        "}\n",
        "\n",
        "data[\"Question\"] =lsq\n",
        "data[\"Ans\"] = lsa\n",
        "\n",
        "#load data into a DataFrame object:\n",
        "df2 = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "df2.to_csv('QandA.csv', index=False)\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"QandA.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c6b805e4-3a9f-4552-beae-2956eeecbf12\", \"QandA.csv\", 650)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "Azk2CvX4vv1y",
        "outputId": "002641eb-9527-499f-b0f2-97ed1284718a"
      },
      "source": [
        "df2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Ans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what is bone cancer</td>\n",
              "      <td>stage 1a bone cancer stage 1b bone cancer stag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what is harmful for health</td>\n",
              "      <td>a 2014 cochrane review found that while asu mi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>are tumors cure</td>\n",
              "      <td>with malignant bone tumors that have not sprea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bye</td>\n",
              "      <td>I am sorry ? No definetive match :p</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Question                                                Ans\n",
              "0         what is bone cancer  stage 1a bone cancer stage 1b bone cancer stag...\n",
              "1  what is harmful for health  a 2014 cochrane review found that while asu mi...\n",
              "2             are tumors cure  with malignant bone tumors that have not sprea...\n",
              "3                         bye                I am sorry ? No definetive match :p"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UwlZ8SCwt4V"
      },
      "source": [
        "def jaccard_similarity(query, document):\n",
        "    intersection = set(query).intersection(set(document))\n",
        "    union = set(query).union(set(document))\n",
        "    return len(intersection)/len(union)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilTD_AI3WqJP",
        "outputId": "694b8b5c-7255-4fde-fe6c-e8e30d019d9b"
      },
      "source": [
        "for i in range(len(df2)):\n",
        "  k = jaccard_similarity(df2.Question[i],df2.Ans[i])\n",
        "  if k<0.4 :\n",
        "    df2.drop([i],axis=0)\n",
        "\n",
        "df2.reset_index()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4583333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HhP09k3XH0g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}